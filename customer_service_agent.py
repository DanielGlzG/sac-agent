"""
Agente de Servicio al Cliente con Strands Agents
==============================================

Este mÃ³dulo implementa un sistema completo de servicio al cliente usando
el patrÃ³n multi-agente de Strands Agents. Incluye agentes especializados
para diferentes aspectos del servicio al cliente.

Autor: ImplementaciÃ³n basada en Strands Agents Framework
Fecha: 2025
"""

import os
import logging
import boto3
from datetime import datetime
from typing import Dict, List, Optional, Any
from strands import Agent, tool
from strands.models.ollama import OllamaModel
from strands.models.bedrock import BedrockModel
from strands.models.openai import OpenAIModel
from strands.models.anthropic import AnthropicModel
from strands_tools import http_request, file_read, file_write
from botocore.client import Config
from botocore.exceptions import ClientError
from dotenv import load_dotenv

# Import prompts
from prompts import SYSTEM_PROMPT_SERVICE_AGENT, SYSTEM_PROMPT_KNOWLEDGE_ASSISTANT, SYSTEM_PROMPT_SERVICE_AGENT_V2

# Import AgentCore context for memory management
try:
    from bedrock_agentcore.runtime.context import RequestContext
    from bedrock_agentcore.memory import MemoryClient
    from strands_tools.agent_core_memory import AgentCoreMemoryToolProvider
except ImportError:
    # Fallback for testing without AgentCore
    RequestContext = None
    MemoryClient = None
    AgentCoreMemoryToolProvider = None

load_dotenv()

# ConfiguraciÃ³n de logging estructurado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s'
)

# Logger principal del agente
logger = logging.getLogger("CustomerService")

# Loggers especÃ­ficos para cada componente
orchestrator_logger = logging.getLogger("CustomerService.Orchestrator")
knowledge_logger = logging.getLogger("CustomerService.Knowledge")
tools_logger = logging.getLogger("CustomerService.Tools")
memory_logger = logging.getLogger("CustomerService.Memory")

# Silenciar logs verbosos de librerÃ­as externas
logging.getLogger("strands").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("strands.telemetry.metrics").setLevel(logging.ERROR)

# ConfiguraciÃ³n del modelo
OLLAMA_CONFIG = {
    "model_id": "gpt-oss:latest",
    "host": "http://localhost:11434",
    "max_tokens": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
}

BEDROCK_CONFIG = {
    "model_id": "anthropic.claude-3-sonnet-20240229-v1:0",
    "region": "us-east-1",
    "connect_timeout": 120,
    "read_timeout": 120,
    "max_attempts": 3
}

OPENAI_CONFIG = {
    "client_args": {
        "api_key": os.getenv("OPENAI_API_KEY"),
    },
    "model_id": "gpt-4o-mini",
    "params": {
        "max_tokens": 1000,
        "temperature": 0.7,
    }
}

# ConfiguraciÃ³n de AWS Bedrock
AWS_CONFIG = {
    "region": "us-west-2",  # Cambiar segÃºn tu regiÃ³n preferida
    "model_id": "anthropic.claude-3-sonnet-20240229-v1:0",  # Solo para referencia, no se usa
    "connect_timeout": 120,
    "read_timeout": 120,
    "max_attempts": 3
}

# ID de la Knowledge Base (debe ser configurado para tu KB especÃ­fica)
KNOWLEDGE_BASE_ID = os.getenv("KNOWLEDGE_BASE_ID")  # Reemplaza con tu Knowledge Base ID real

# ConfiguraciÃ³n de AgentCore Memory
# Verificar que las variables de entorno estÃ©n configuradas
_memory_id = os.getenv("AGENTCORE_MEMORY_ID")
_region = os.getenv("AWS_REGION")
_user_pref_strategy = os.getenv("MEMORY_STRATEGY_USER_PREFERENCES")
_summaries_strategy = os.getenv("MEMORY_STRATEGY_SUMMARIES")
_semantic_strategy = os.getenv("MEMORY_STRATEGY_SEMANTIC")



MEMORY_CONFIG = {
    "memory_id": _memory_id,
    "region": _region,
    "actor_id_prefix": "customer_",  # Prefijo para identificar usuarios
    "namespaces": {
        "user_preferences": f"/strategies/{_user_pref_strategy}/actors/{{actor_id}}",
        "conversation_summaries": f"/strategies/{_summaries_strategy}/actors/{{actor_id}}/sessions/{{session_id}}",
        "semantic_memory": f"/strategies/{_semantic_strategy}/actors/{{actor_id}}"
    }
}

class BedrockKnowledgeBaseClient:
    """Cliente para interactuar con AWS Bedrock Knowledge Base (solo para recuperaciÃ³n)."""
    
    def __init__(self, region: str = None, knowledge_base_id: str = None):
        self.region = region or AWS_CONFIG["region"]
        self.knowledge_base_id = knowledge_base_id or KNOWLEDGE_BASE_ID
        
        # ConfiguraciÃ³n de cliente boto3
        bedrock_config = Config(
            connect_timeout=AWS_CONFIG["connect_timeout"],
            read_timeout=AWS_CONFIG["read_timeout"],
            retries={'max_attempts': AWS_CONFIG["max_attempts"]}
        )
        
        try:
            # Cliente para operaciones de runtime (consultas)
            self.bedrock_agent_runtime = boto3.client(
                "bedrock-agent-runtime",
                region_name=self.region,
                config=bedrock_config
            )
            
            
            logger.info(f"âœ… Cliente Bedrock inicializado para regiÃ³n: {self.region}")
            
        except Exception as e:
            logger.error(f"âŒ Error inicializando cliente Bedrock: {e}")
            raise
    
    def retrieve(self, query: str, max_results: int = 20, min_score: float = 0.1) -> Dict[str, Any]:
        """
        Recupera informaciÃ³n relevante de la Knowledge Base sin generar respuesta.
        
        Args:
            query: Consulta a realizar
            max_results: NÃºmero mÃ¡ximo de resultados
            min_score: PuntuaciÃ³n mÃ­nima de relevancia
            
        Returns:
            Diccionario con los resultados de la bÃºsqueda
        """
        try:
            logger.info(f"ğŸ” BEDROCK RETRIEVE: '{query}' (max={max_results}, min_score={min_score})")
            
            response = self.bedrock_agent_runtime.retrieve(
                knowledgeBaseId=self.knowledge_base_id,
                retrievalQuery={
                    'text': query
                },
                retrievalConfiguration={
                    'vectorSearchConfiguration': {
                        'numberOfResults': max_results,
                    }
                }
            )
            
            # Procesar y filtrar resultados por score
            results = []
            for result in response.get('retrievalResults', []):
                score = result.get('score', 0.0)
                if score >= min_score:
                    results.append({
                        'content': result.get('content', {}).get('text', ''),
                        'score': score,
                        'location': result.get('location', {}),
                        'metadata': result.get('metadata', {})
                    })
            
            # Ordenar por score descendente
            results.sort(key=lambda x: x['score'], reverse=True)
            
            logger.info(f"ğŸ“Š BEDROCK RETRIEVE RESULTS: {len(results)} resultados encontrados")
            
            return {
                'results': results,
                'total_results': len(results),
                'query': query,
                'knowledge_base_id': self.knowledge_base_id
            }
            
        except ClientError as e:
            logger.error(f"âŒ Error en Bedrock retrieve: {e}")
            return {
                'results': [],
                'total_results': 0,
                'query': query,
                'error': str(e)
            }
        except Exception as e:
            logger.error(f"âŒ Error inesperado en retrieve: {e}")
            return {
                'results': [],
                'total_results': 0,
                'query': query,
                'error': str(e)
            }

# Instancia global del cliente Bedrock
bedrock_client = BedrockKnowledgeBaseClient()

class AgentCoreMemoryManager:
    """Gestor de memoria para AgentCore con estrategias de corto y largo plazo."""
    
    def __init__(self, memory_id: str = None, region: str = None):
        self.memory_id = memory_id or MEMORY_CONFIG["memory_id"]
        self.region = region or MEMORY_CONFIG["region"]
        self.actor_id_prefix = MEMORY_CONFIG["actor_id_prefix"]
        self.namespaces = MEMORY_CONFIG["namespaces"]
        
        # Inicializar cliente de memoria si estÃ¡ disponible
        if MemoryClient:
            try:
                self.memory_client = MemoryClient(region_name=self.region)
                memory_logger.info(f"ğŸ§  AgentCore Memory initialized | Memory ID: {self.memory_id} | Region: {self.region}")
            except Exception as e:
                memory_logger.error(f"ğŸ§  Failed to initialize AgentCore Memory: {e}")
                self.memory_client = None
        else:
            self.memory_client = None
            memory_logger.warning("ğŸ§  AgentCore Memory not available (import failed)")
    
    def format_actor_id(self, user_id: str) -> str:
        """Formato consistente para actor_id."""
        return f"{self.actor_id_prefix}{user_id}"
    
    def create_event(self, actor_id: str, session_id: str, user_message: str, agent_response: str) -> bool:
        """
        Guarda un evento (turno de conversaciÃ³n) en memoria de corto plazo.
        
        Args:
            actor_id: ID del actor (usuario)
            session_id: ID de la sesiÃ³n de conversaciÃ³n
            user_message: Mensaje del usuario
            agent_response: Respuesta del agente
            
        Returns:
            bool: True si se guardÃ³ exitosamente
        """
        if not self.memory_client:
            memory_logger.warning("ğŸ§  Cannot create event - Memory client not available")
            return False
        
        try:
            formatted_actor_id = self.format_actor_id(actor_id)
            memory_logger.info(f"ğŸ” CREATE EVENT: actor_id={actor_id} -> formatted={formatted_actor_id}")
            
            # Crear evento con mensajes del turno
            self.memory_client.create_event(
                memory_id=self.memory_id,
                actor_id=formatted_actor_id,
                session_id=session_id,
                messages=[
                    (user_message, "USER"),
                    (agent_response, "ASSISTANT")
                ]
            )
            
            memory_logger.info(f"ğŸ§  Event created | Actor: {formatted_actor_id} | Session: {session_id}")
            return True
            
        except Exception as e:
            memory_logger.error(f"ğŸ§  Failed to create event: {e}")
            return False
    
    def retrieve_memories(self, actor_id: str, session_id: str = None, namespace_type: str = "user_preferences") -> Dict[str, Any]:
        """
        Recupera memorias de largo plazo usando las estrategias configuradas.
        
        Args:
            actor_id: ID del actor (usuario)
            session_id: ID de sesiÃ³n (opcional, para summaries)
            namespace_type: Tipo de namespace ("user_preferences", "conversation_summaries", "semantic_memory")
            
        Returns:
            Dict con las memorias recuperadas
        """
        if not self.memory_client:
            memory_logger.warning("ğŸ§  Cannot retrieve memories - Memory client not available")
            return {"memories": [], "total": 0}
        
        try:
            formatted_actor_id = self.format_actor_id(actor_id)
            
            # Obtener el namespace template directamente
            namespace_template = self.namespaces.get(namespace_type)
            if not namespace_template:
                memory_logger.error(f"ğŸ§  Namespace template not found for: {namespace_type}")
                return {"memories": [], "total": 0}
            
            # Construir namespace reemplazando las variables
            if namespace_type == "conversation_summaries" and session_id:
                namespace = namespace_template.format(actor_id=formatted_actor_id, session_id=session_id)
            else:
                namespace = namespace_template.format(actor_id=formatted_actor_id)
            
            # Recuperar memorias del namespace especÃ­fico
            response = self.memory_client.retrieve_memories(
                memory_id=self.memory_id,
                query="user information",  # Query genÃ©rica para obtener memorias del namespace
                namespace=namespace
            )
            
            # La respuesta puede ser una lista directa o un dict con 'memories'
            if isinstance(response, list):
                memories = response
            else:
                memories = response.get("memories", [])
            
            memory_logger.info(f"ğŸ§  Memories retrieved | Actor: {formatted_actor_id} | Namespace: {namespace} | Count: {len(memories)}")
            
            return {
                "memories": memories,
                "total": len(memories),
                "namespace": namespace,
                "actor_id": formatted_actor_id
            }
            
        except Exception as e:
            memory_logger.error(f"ğŸ§  Failed to retrieve memories: {e}")
            return {"memories": [], "total": 0, "error": str(e)}
    
    def get_user_context(self, actor_id: str, session_id: str) -> str:
        """
        Construye contexto del usuario combinando diferentes tipos de memoria.
        
        Args:
            actor_id: ID del actor (usuario)
            session_id: ID de sesiÃ³n
            
        Returns:
            str: Contexto formateado para el agente
        """
        context_parts = []
        
        try:
            # Recuperar preferencias del usuario
            user_prefs = self.retrieve_memories(actor_id, namespace_type="user_preferences")
            memory_logger.info(f"ğŸ§  USER CONTEXT: Retrieved user preferences - {user_prefs['total']} items")
            
            if user_prefs["memories"]:
                context_parts.append("ğŸ‘¤ **InformaciÃ³n del usuario:**")
                for memory in user_prefs["memories"][:3]:  # MÃ¡ximo 3 preferencias
                    content = memory.get('content', '') if isinstance(memory, dict) else str(memory)
                    if content:
                        context_parts.append(f"- {content}")
            
            # Recuperar resÃºmenes de conversaciones anteriores (solo si hay session_id)
            if session_id:
                summaries = self.retrieve_memories(actor_id, session_id, "conversation_summaries")
                memory_logger.info(f"ğŸ§  USER CONTEXT: Retrieved conversation summaries - {summaries['total']} items")
                
                if summaries["memories"]:
                    context_parts.append("\nğŸ“ **Conversaciones anteriores:**")
                    for memory in summaries["memories"][:2]:  # MÃ¡ximo 2 resÃºmenes
                        content = memory.get('content', '') if isinstance(memory, dict) else str(memory)
                        if content:
                            context_parts.append(f"- {content}")
            
            # Recuperar memoria semÃ¡ntica
            semantic = self.retrieve_memories(actor_id, namespace_type="semantic_memory")
            memory_logger.info(f"ğŸ§  USER CONTEXT: Retrieved semantic memory - {semantic['total']} items")
            
            if semantic["memories"]:
                context_parts.append("\nğŸ§  **Contexto relacionado:**")
                for memory in semantic["memories"][:2]:  # MÃ¡ximo 2 elementos semÃ¡nticos
                    content = memory.get('content', '') if isinstance(memory, dict) else str(memory)
                    if content:
                        context_parts.append(f"- {content}")
            
            final_context = "\n".join(context_parts) + "\n\n" if context_parts else ""
            memory_logger.info(f"ğŸ§  USER CONTEXT: Built context with {len(context_parts)} parts | Length: {len(final_context)} chars")
            
            return final_context
            
        except Exception as e:
            memory_logger.error(f"ğŸ§  USER CONTEXT ERROR: {e}")
            return ""

# Instancia global del gestor de memoria
memory_manager = AgentCoreMemoryManager()

def create_model_ollama():
    """Crea y retorna una instancia del modelo configurado."""
    return OllamaModel(**OLLAMA_CONFIG)

def create_model_bedrock():
    """Crea y retorna una instancia del modelo configurado."""
    return BedrockModel(**BEDROCK_CONFIG)

def create_model_openai():
    """Crea y retorna una instancia del modelo configurado."""
    return OpenAIModel(client_args={
        "api_key": os.getenv("OPENAI_API_KEY"),
    },
    model_id="gpt-4o-mini",
    params={
        "max_tokens": 1000,
        "temperature": 0.7,
    })

def create_model_anthropic():
    """Crea y retorna una instancia del modelo configurado."""
    return AnthropicModel(**ANTHROPIC_CONFIG)

# ==========================================
# HERRAMIENTAS PERSONALIZADAS
# ==========================================

@tool
def get_current_time() -> str:
    """
    Obtiene la fecha y hora actual.
    
    Returns:
        str: Fecha y hora actual en formato legible
    """
    tools_logger.info(f"ğŸ”§ TOOL INPUT: get_current_time | Params: none")
    tools_logger.info(f"ğŸ”§ TOOL PROCESSING: get_current_time | Action: Getting current system time")
    result = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    tools_logger.info(f"ğŸ”§ TOOL OUTPUT: get_current_time | Result: {result}")
    return result

@tool
def search_knowledge_base(query: str) -> str:
    """
    Busca informaciÃ³n en la base de conocimientos de AWS Bedrock.
    
    Args:
        query: Consulta de bÃºsqueda
        
    Returns:
        str: InformaciÃ³n relevante encontrada
    """
    tools_logger.info(f"ğŸ”§ TOOL INPUT: search_knowledge_base | Params: query='{query}'")
    
    try:
        tools_logger.info(f"ğŸ”§ TOOL PROCESSING: search_knowledge_base | Action: Calling Bedrock Knowledge Base retrieve")
        
        # BÃºsqueda en Bedrock Knowledge Base

        results = bedrock_client.retrieve(query, max_results=15, min_score=0.25)
        
        if results.get('error'):
            tools_logger.error(f"ğŸ”§ TOOL ERROR: search_knowledge_base | Bedrock error: {results['error']}")
        
        if not results['results']:
            tools_logger.info(f"ğŸ”§ TOOL PROCESSING: search_knowledge_base | Action: No results found in Bedrock, falling back")
        
        # Formatear resultados de Bedrock
        formatted_results = []
        total_results = len(results['results'])
        
        # Tomar los mejores resultados (mÃ¡ximo 3 para no sobrecargar)
        top_results = results['results'][:3]
        
        tools_logger.info(f"ğŸ”§ TOOL PROCESSING: search_knowledge_base | Action: Formatting {len(top_results)} top results from {total_results} total")
        
        for i, result in enumerate(top_results, 1):
            score = result['score']
            content = result['content']
            
            # Limitar longitud del contenido
            content_preview = content[:800] + "..." if len(content) > 800 else content
            
            formatted_results.append(
                f"ğŸ“„ **Resultado {i}** (relevancia: {score:.2f})\n{content_preview}"
            )
        
        formatted_response = f"ğŸ” **InformaciÃ³n encontrada en la base de conocimientos:**\n\n"
        formatted_response += f"ğŸ“Š **{total_results} resultados** para: \"{query}\"\n\n"
        formatted_response += "\n\n".join(formatted_results)
        
        if total_results > 3:
            formatted_response += f"\n\nğŸ’¡ *Se encontraron {total_results - 3} resultados adicionales. Puedes hacer una consulta mÃ¡s especÃ­fica para obtener informaciÃ³n mÃ¡s precisa.*"
        
        tools_logger.info(f"ğŸ”§ TOOL OUTPUT: search_knowledge_base | Result: Found {total_results} results from Bedrock | Response length: {len(formatted_response)} chars")
        return formatted_response
        
    except Exception as e:
        tools_logger.error(f"ğŸ”§ TOOL ERROR: search_knowledge_base | Exception: {e}")

@tool
def escalate_to_human(reason: str, customer_info: str = "") -> str:
    """
    Escala la conversaciÃ³n a un agente humano.
    
    Args:
        reason: Motivo de la escalaciÃ³n
        customer_info: InformaciÃ³n adicional del cliente
        
    Returns:
        str: ConfirmaciÃ³n de escalaciÃ³n
    """
    tools_logger.info(f"ğŸ”§ TOOL INPUT: escalate_to_human | Params: reason='{reason}', customer_info='{customer_info[:100]}...' ")
    
    escalation_id = datetime.now().strftime("%Y%m%d%H%M%S")
    
    tools_logger.info(f"ğŸ”§ TOOL PROCESSING: escalate_to_human | Action: Creating escalation with ID {escalation_id}")
    
    # En producciÃ³n, esto activarÃ­a el sistema de routing humano
    # AquÃ­ se harÃ­a la llamada al sistema de tickets/escalaciÃ³n
    tools_logger.info(f"ğŸ”§ TOOL PROCESSING: escalate_to_human | Action: Would trigger human routing system (simulated)")
    
    escalation_response = f"""ğŸš€ **Escalando a agente humano**

    ğŸ†” **ID de EscalaciÃ³n**: {escalation_id}
    ğŸ“ **Motivo**: {reason}
    â±ï¸ **Tiempo estimado de espera**: 3-5 minutos

    Un agente humano se conectarÃ¡ contigo pronto. Mientras tanto:
    - MantÃ©n esta ventana abierta
    - Ten a la mano cualquier informaciÃ³n relevante
    - Si necesitas cerrar, puedes retomar la conversaciÃ³n citando el ID: {escalation_id}

    ğŸ§ TambiÃ©n puedes llamar directamente al +1-234-567-8900"""
    
    tools_logger.info(f"ğŸ”§ TOOL OUTPUT: escalate_to_human | Result: Escalation created with ID {escalation_id} | Response length: {len(escalation_response)} chars")
    
    return escalation_response


# ==========================================
# AGENTES ESPECIALIZADOS
# ==========================================

@tool
def knowledge_assistant(query: str) -> str:
    """
    Asistente especializado en consultas generales y FAQ.
    
    Args:
        query: Consulta sobre informaciÃ³n general de la empresa
        
    Returns:
        str: Respuesta basada en la base de conocimientos
    """
    knowledge_logger.info(f"ğŸ¤– AGENT INPUT: knowledge_assistant | Message: '{query}'")
    
    try:
        knowledge_logger.info(f"ğŸ¤– AGENT PROCESSING: knowledge_assistant | Decision: Creating specialized knowledge agent")
        
        agent = Agent(
            model=create_model_openai(),
            system_prompt=SYSTEM_PROMPT_KNOWLEDGE_ASSISTANT,
            tools=[search_knowledge_base, get_current_time]
        )
        
        knowledge_logger.info(f"ğŸ¤– AGENT PROCESSING: knowledge_assistant | Decision: Executing query with knowledge base tools")
        
        response = agent(f"Responde esta consulta usando la base de conocimientos: {query}")
        
        knowledge_logger.info(f"ğŸ¤– AGENT OUTPUT: knowledge_assistant | Response: Length={len(str(response))} chars, Preview='{str(response)[:100]}...'")
        return str(response)
        
    except Exception as e:
        knowledge_logger.error(f"ğŸ¤– AGENT ERROR: knowledge_assistant | Exception: {e}")
        error_response = f"âŒ Error al procesar tu consulta. Por favor contacta a soporte: {str(e)}"
        knowledge_logger.info(f"ğŸ¤– AGENT OUTPUT: knowledge_assistant | Response: Error response due to exception")
        return error_response


# ==========================================
# AGENTE PRINCIPAL (ORCHESTRATOR)
# ==========================================

class CustomerServiceOrchestrator:
    """Agente principal que coordina todos los asistentes especializados con AgentCore Memory."""
    
    def __init__(self, context: Optional['RequestContext'] = None):
        self.model = create_model_openai()
        self.context = context
        self.session_id = context.session_id if context else f"local_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Configurar memoria nativa de AgentCore si estÃ¡ disponible
        if AgentCoreMemoryToolProvider and MEMORY_CONFIG["memory_id"]:
            try:
                # Crear provider de memoria usando el namespace de preferencias
                namespace = MEMORY_CONFIG["namespaces"]["user_preferences"]
                
                self.memory_provider = AgentCoreMemoryToolProvider(
                    memory_id=MEMORY_CONFIG["memory_id"],
                    actor_id=f"customer_agent_{self.session_id}",  # Actor ID Ãºnico por sesiÃ³n
                    session_id=self.session_id,
                    namespace=namespace,  # Namespace ya configurado desde .env
                    region=MEMORY_CONFIG["region"]
                )
                memory_logger.info(f"ğŸ§  ORCHESTRATOR INIT: AgentCore Memory provider enabled | Session: {self.session_id}")
            except Exception as e:
                memory_logger.error(f"ğŸ§  ORCHESTRATOR INIT: Failed to setup memory provider: {e}")
                self.memory_provider = None
        else:
            self.memory_provider = None
            memory_logger.info(f"ğŸ§  ORCHESTRATOR INIT: AgentCore Memory provider disabled | Session: {self.session_id}")
        
        # Prompt del sistema optimizado para servicio al cliente con Bedrock y memoria
        self.system_prompt = SYSTEM_PROMPT_SERVICE_AGENT_V2

        # Crear el agente principal con herramientas (incluyendo memoria si estÃ¡ disponible)
        tools = [knowledge_assistant]
        if self.memory_provider and hasattr(self.memory_provider, 'tools'):
            tools.extend(self.memory_provider.tools)
            
        self.orchestrator = Agent(
            model=self.model,
            system_prompt=self.system_prompt,
            tools=tools
        )
        
        memory_tools_count = len(self.memory_provider.tools) if self.memory_provider and hasattr(self.memory_provider, 'tools') else 0
        memory_logger.info(f"ğŸ§  ORCHESTRATOR INIT: Agent created | Total tools: {len(tools)} | Memory tools: {memory_tools_count} | Provider: {'âœ…' if self.memory_provider else 'âŒ'}")
    
    def chat(self, message: str, user_id: str = None) -> str:
        """
        Procesa un mensaje del cliente y retorna la respuesta apropiada usando AgentCore Memory.
        
        Args:
            message: Mensaje del cliente
            user_id: ID del usuario para contexto personalizado
            
        Returns:
            str: Respuesta del agente de servicio al cliente
        """
        orchestrator_logger.info(f"ğŸ¯ ORCHESTRATOR INPUT: '{message}' | User: {user_id} | Session: {self.session_id}")
        
        try:
            # Obtener contexto del usuario desde AgentCore Memory
            user_context = ""
            if user_id and self.session_id:
                user_context = memory_manager.get_user_context(user_id, self.session_id)
            
            # Recuperar historial de conversaciÃ³n local (para compatibilidad)
            conversation_history = self._get_conversation_history()
            
            # Construir contexto completo del mensaje incluyendo memoria de largo plazo
            contextualized_message = self._build_contextualized_message(message, user_id, conversation_history, user_context)
            
            # Procesar mensaje con el orchestrator (que ahora tiene herramientas de memoria)
            response = self.orchestrator(contextualized_message)
            
            # Guardar interacciÃ³n en memoria local (para compatibilidad)
            self._save_interaction(message, str(response), user_id)
            
            # Guardar evento en AgentCore Memory (short-term -> long-term extraction)
            if user_id and self.session_id:
                memory_logger.info(f"ğŸ” SAVING EVENT: user_id={user_id}, session={self.session_id}")
                memory_logger.info(f"ğŸ” USER MESSAGE: {message}")
                memory_logger.info(f"ğŸ” AGENT RESPONSE: {str(response)[:200]}...")
                # Usar user_id directamente (el mÃ©todo create_event formatea internamente)
                memory_manager.create_event(user_id, self.session_id, message, str(response))
            
            orchestrator_logger.info(f"ğŸ¯ ORCHESTRATOR OUTPUT: Length: {len(str(response))} chars | Preview: {str(response)[:100]}...")
            return str(response)
            
        except Exception as e:
            logger.error(f"Error en chat: {e}")
            error_response = f"""âŒ **Error del sistema**

            Disculpa, he encontrado un problema tÃ©cnico. 

            ğŸ”§ **Opciones disponibles:**
            1. Intenta reformular tu consulta
            2. Contacta directamente: +1-234-567-8900
            3. Email: soporte@empresa.com

            **Error**: {str(e)}"""
            return error_response
    
    def _get_conversation_history(self) -> List[Dict[str, Any]]:
        """Recupera el historial de conversaciÃ³n desde AgentCore Memory."""
        if not self.context or not hasattr(self.context, 'memory'):
            memory_logger.info("ğŸ§  MEMORY: No context available, starting fresh conversation")
            return []
        
        try:
            history = self.context.memory.get('conversation_history', [])
            memory_logger.info(f"ğŸ§  MEMORY GET: Retrieved {len(history)} previous interactions")
            return history
        except Exception as e:
            memory_logger.error(f"ğŸ§  MEMORY ERROR: Failed to retrieve history - {e}")
            return []
    
    def _save_interaction(self, user_message: str, agent_response: str, user_id: str = None):
        """Guarda la interacciÃ³n actual en AgentCore Memory."""
        if not self.context or not hasattr(self.context, 'memory'):
            memory_logger.warning("ğŸ§  MEMORY: No context available, cannot save interaction")
            return
        
        try:
            # Obtener historial actual
            history = self._get_conversation_history()
            
            # AÃ±adir nueva interacciÃ³n
            new_interaction = {
                'timestamp': datetime.now().isoformat(),
                'user_id': user_id,
                'user_message': user_message,
                'agent_response': agent_response,
                'session_id': self.session_id
            }
            
            history.append(new_interaction)
            
            # Limitar historial a las Ãºltimas 10 interacciones para evitar overflow
            if len(history) > 10:
                history = history[-10:]
            
            # Guardar en memoria
            self.context.memory.set('conversation_history', history)
            
            memory_logger.info(f"ğŸ§  MEMORY SET: Saved interaction | Total history: {len(history)} items")
            
        except Exception as e:
            memory_logger.error(f"ğŸ§  MEMORY ERROR: Failed to save interaction - {e}")
    
    def _build_contextualized_message(self, message: str, user_id: str = None, history: List[Dict] = None, user_context: str = "") -> str:
        """Construye un mensaje contextualizado con historial de conversaciÃ³n y memoria de largo plazo."""
        context_parts = []
        
        # AÃ±adir contexto de memoria de largo plazo si estÃ¡ disponible
        if user_context:
            context_parts.append("ğŸ§  **Memoria de largo plazo del usuario:**")
            context_parts.append(user_context)
        
        # AÃ±adir historial de sesiÃ³n reciente si existe
        if history:
            # AÃ±adir resumen del historial reciente (hasta 3 interacciones anteriores)
            recent_history = history[-3:] if len(history) > 3 else history
            
            if recent_history:
                context_parts.append("ğŸ“ **Contexto de conversaciÃ³n actual:**")
                for i, interaction in enumerate(recent_history, 1):
                    context_parts.append(f"InteracciÃ³n {i}:")
                    context_parts.append(f"Usuario: {interaction.get('user_message', 'N/A')}")
                    context_parts.append(f"Agente: {interaction.get('agent_response', 'N/A')[:100]}...")
                    context_parts.append("")
        
        # AÃ±adir mensaje actual (sin incluir user_id para evitar propagaciÃ³n)
        context_parts.append("ğŸ’¬ **Mensaje actual:**")
        context_parts.append(message)
        
        contextualized_message = "\n".join(context_parts)
        
        has_long_term = bool(user_context)
        has_history = bool(history)
        memory_logger.info(f"ğŸ§  CONTEXT: Built contextualized message | Long-term: {has_long_term} | History: {len(history) if history else 0} items")
        
        return contextualized_message

def main():
    """FunciÃ³n principal para probar el agente de servicio al cliente."""
    
    print("ğŸ¤– " + "="*60)
    print("   AGENTE DE SERVICIO AL CLIENTE - STRANDS AGENTS")
    print("="*60)
    print("ğŸ’¡ Escribe 'exit' para salir del chat")
    print("ğŸ’¡ Escribe 'help' para ver comandos disponibles")
    print("-"*60)
    
    # Crear instancia del orchestrator (sin context para testing local)
    agent = CustomerServiceOrchestrator()
    
    # Mensaje de bienvenida
    welcome_message = """Â¡Hola! ğŸ‘‹ Soy tu asistente de servicio al cliente con acceso a informaciÃ³n actualizada.

    ğŸ¯ **Â¿En quÃ© puedo ayudarte hoy?**

    Puedo asistirte con:
    ğŸ“š InformaciÃ³n general y FAQ (desde base de conocimientos empresarial)
    ğŸ”§ Problemas tÃ©cnicos y soporte especializado
    ğŸ’¼ Consultas sobre productos y ventas  
    ğŸ“¦ Estado de Ã³rdenes y envÃ­os
    â˜ï¸ Consultas avanzadas en documentaciÃ³n tÃ©cnica
    ğŸš€ Y mucho mÃ¡s...

    âœ¨ **Conectado a AWS Bedrock Knowledge Base** para informaciÃ³n mÃ¡s precisa y actualizada.

    Solo describe tu consulta y te conectarÃ© con el asistente especializado apropiado."""
    
    print(f"\nğŸ¤– {welcome_message}\n")
    
    # Loop principal del chat
    while True:
        try:
            # Obtener input del usuario
            user_input = input("ğŸ‘¤ TÃº: ").strip()
            
            # Comandos especiales
            if user_input.lower() == 'exit':
                print("\nğŸ‘‹ Â¡Gracias por usar nuestro servicio! Que tengas un excelente dÃ­a.")
                break
            elif user_input.lower() == 'help':
                print("""
                ğŸ†˜ **Comandos disponibles:**
                - 'exit': Salir del chat
                - 'help': Mostrar esta ayuda
                - Cualquier otra consulta serÃ¡ procesada por el agente

                ğŸ“ **Contacto directo:**
                - TelÃ©fono: +1-234-567-8900
                - Email: soporte@empresa.com
                """)
                continue
            elif not user_input:
                continue
            
            # Procesar mensaje con el agente (con user_id de prueba para testing local)
            response = agent.chat(user_input, user_id="local_test_user")
            print(f"\nğŸ¤– {response}\n")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ConversaciÃ³n interrumpida. Â¡Hasta la prÃ³xima!")
            break
        except Exception as e:
            print(f"\nâŒ Error inesperado: {e}")
            print("ğŸ”§ Por favor intenta de nuevo o contacta a soporte tÃ©cnico.\n")

if __name__ == "__main__":
    main()

